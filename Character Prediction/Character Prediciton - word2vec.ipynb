{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Prediction\n",
    "\n",
    "## Word to Vector and Bag of Words dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marcu\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data \n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for y dataset and common labels\n",
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"ARAGORN\"]\n",
    "\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "def y_split(data):\n",
    "    y_data = data['char']\n",
    "    return y_data\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = common_label_removal(data)\n",
    "    y_data = y_split(data)\n",
    "    y_data = char_2_num(y_data)\n",
    "    return pd.Series(y_data)\n",
    "\n",
    "train_y = preprocessing(train_df)\n",
    "val_y = preprocessing(val_df)\n",
    "test_y = preprocessing(test_df)\n",
    "\n",
    "train_y.to_csv('train_y.csv', index=False)\n",
    "val_y.to_csv('val_y.csv', index=False)\n",
    "test_y.to_csv('test_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vector Dataset\n",
    "\n",
    "This idea was ultimatrly not used but was still apart of our research process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def find_max_length(data):\n",
    "#     return max(max(len(nlp(dialogue)) for dialogue in set) for set in data)\n",
    "\n",
    "# max_length = find_max_length([train_df['dialog'], val_df['dialog'], test_df['dialog']])\n",
    "\n",
    "# def word2vec_seq(data, max_length):\n",
    "#     # Extracting norm vector values\n",
    "#     word_vectors = []\n",
    "#     for dialogue in data['dialog']:\n",
    "#         tokens = nlp(dialogue)\n",
    "#         dialogue_vectors = [token.vector_norm for token in tokens]\n",
    "#         word_vectors.append(dialogue_vectors)\n",
    "\n",
    "#     # Padding \n",
    "#     for i in range(len(word_vectors)):\n",
    "#         word_vectors[i] += [100] * (max_length - len(word_vectors[i]))\n",
    "\n",
    "#     df = pd.DataFrame(word_vectors)\n",
    "#     df.columns = [f\"word_{i}\" for i in range(1, max_length + 1)]\n",
    "\n",
    "#     df = pd.concat([data, df], axis=1)\n",
    "#     return df  \n",
    "\n",
    "# train_B = word2vec_seq(train_df, max_length)\n",
    "# val_B = word2vec_seq(val_df, max_length)\n",
    "# test_B = word2vec_seq(test_df, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_B.to_csv('train_B.csv', index=False)\n",
    "# val_B.to_csv('val_B.csv', index=False)\n",
    "# test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = common_label_removal(train_df).reset_index(drop=True)\n",
    "val_B = common_label_removal(val_df).reset_index(drop=True)\n",
    "test_B = common_label_removal(test_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_list(X):\n",
    "    quote_list = []\n",
    "    for quote in range(len(X)):\n",
    "        splitted_quote =  X['dialog'][quote].split()\n",
    "        sequence_list = []\n",
    "        for split in range(len(splitted_quote)):\n",
    "            splitted_word = splitted_quote[split]\n",
    "\n",
    "            word_list = str()\n",
    "            i=0\n",
    "            while i < (len(splitted_word)):\n",
    "                # print(splitted_word[i])|\n",
    "                if splitted_word[i].isalpha() == True:\n",
    "                    word_list += splitted_word[i]\n",
    "                i+=1\n",
    "            sequence_list.append(word_list)\n",
    "        quote_list.append(sequence_list)\n",
    "    return quote_list\n",
    "\n",
    "B1 = quote_list(train_B)\n",
    "B2 = quote_list(val_B)\n",
    "B3 = quote_list(test_B)\n",
    "\n",
    "def maxlen(X):\n",
    "    uni = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            if X[i][j] not in uni:\n",
    "                uni.append(X[i][j])\n",
    "    return len(uni)\n",
    "max_length = maxlen(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(B1)\n",
    "B1_seq = tokenizer.texts_to_sequences(B1)\n",
    "B2_seq = tokenizer.texts_to_sequences(B2)\n",
    "B3_seq = tokenizer.texts_to_sequences(B3)\n",
    "maxlen = max([len(seq) for seq in B1_seq])\n",
    "\n",
    "B1_padseq = pad_sequences(B1_seq, maxlen=max_length,padding='post')\n",
    "B2_padseq = pad_sequences(B2_seq, maxlen=maxlen,padding='post')\n",
    "B3_padseq = pad_sequences(B3_seq, maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = pd.DataFrame(B1_padseq)\n",
    "val_B = pd.DataFrame(B2_padseq)\n",
    "test_B = pd.DataFrame(B3_padseq)\n",
    "\n",
    "train_B.to_csv('train_B.csv', index=False)\n",
    "val_B.to_csv('val_B.csv', index=False)\n",
    "test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Removing; punctuation, stopwords and stemming words\n",
    "def bow_preprocess(df):\n",
    "    preprocessed = []\n",
    "    \n",
    "    for dialog in df['dialog']:\n",
    "        tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \", dialog).split()\n",
    "        alfa = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords_set]\n",
    "        bravo = str()\n",
    "        for i in range(len(alfa)):\n",
    "            bravo += str(alfa[i])\n",
    "            bravo += ' '\n",
    "        preprocessed.append(bravo)\n",
    "\n",
    "    df['dialog'] = preprocessed\n",
    "    return df\n",
    "\n",
    "\n",
    "train_C = bow_preprocess(train_df)\n",
    "val_C = bow_preprocess(val_df)\n",
    "test_C = bow_preprocess(test_df)\n",
    "\n",
    "# Creating a set of unique words amongst all dialoges\n",
    "all_dialogs = pd.concat([train_C['dialog'], val_C['dialog'], test_C['dialog']], ignore_index=True)\n",
    "wordset = set([word for dialog in all_dialogs for word in dialog.split()])\n",
    "\n",
    "# Creating dataframe\n",
    "def BOW_df(wordset, df):\n",
    "\n",
    "    bow_df = pd.DataFrame(columns=list(wordset))\n",
    "    \n",
    "    for i, dialog in enumerate(df['dialog']):\n",
    "        dialog_tf_diz = {}\n",
    "        for word in dialog.split():\n",
    "            if word in wordset:\n",
    "                if word in dialog_tf_diz:\n",
    "                    dialog_tf_diz[word] += 1\n",
    "                else:\n",
    "                    dialog_tf_diz[word] = 1\n",
    "        \n",
    "        bow_df.loc[i] = [dialog_tf_diz.get(word, 0) for word in wordset]\n",
    "    return pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "train_C = BOW_df(wordset, train_C)\n",
    "val_C = BOW_df(wordset, val_C)\n",
    "test_C = BOW_df(wordset, test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_C = common_label_removal(train_C).reset_index(drop=True).iloc[:,2:]\n",
    "val_C = common_label_removal(val_C).reset_index(drop=True).iloc[:,2:]\n",
    "test_C = common_label_removal(test_C).reset_index(drop=True).iloc[:,2:]\n",
    "\n",
    "train_C.to_csv('train_C.csv', index=False)\n",
    "val_C.to_csv('val_C.csv', index=False)\n",
    "test_C.to_csv('test_C.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_idf__df(df):\n",
    "    preprocessed_dialogs = bow_preprocess(df)\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=wordset)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_dialogs['dialog'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=list(wordset))\n",
    "    return pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "train_D = tf_idf__df(train_df)\n",
    "val_D = tf_idf__df(val_df)\n",
    "test_D = tf_idf__df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = common_label_removal(train_D).reset_index(drop=True).iloc[:,2:]\n",
    "val_D = common_label_removal(val_D).reset_index(drop=True).iloc[:,2:]\n",
    "test_D = common_label_removal(test_D).reset_index(drop=True).iloc[:,2:]\n",
    "\n",
    "train_D.to_csv('train_D.csv', index=False)\n",
    "val_D.to_csv('val_D.csv', index=False)\n",
    "test_D.to_csv('test_D.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>draught</th>\n",
       "      <th>broil</th>\n",
       "      <th>ra</th>\n",
       "      <th>flee</th>\n",
       "      <th>holiday</th>\n",
       "      <th>ril</th>\n",
       "      <th>say</th>\n",
       "      <th>youshouldn</th>\n",
       "      <th>town</th>\n",
       "      <th>start</th>\n",
       "      <th>...</th>\n",
       "      <th>outof</th>\n",
       "      <th>heathen</th>\n",
       "      <th>pike</th>\n",
       "      <th>speed</th>\n",
       "      <th>wonder</th>\n",
       "      <th>7</th>\n",
       "      <th>withdraw</th>\n",
       "      <th>gold</th>\n",
       "      <th>green</th>\n",
       "      <th>silenc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1147 rows Ã— 2697 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      draught  broil   ra  flee  holiday  ril  say  youshouldn  town  start  \\\n",
       "0         0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "1         0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "2         0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "3         0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "4         0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "...       ...    ...  ...   ...      ...  ...  ...         ...   ...    ...   \n",
       "1142      0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "1143      0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "1144      0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "1145      0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "1146      0.0    0.0  0.0   0.0      0.0  0.0  0.0         0.0   0.0    0.0   \n",
       "\n",
       "      ...  outof  heathen  pike  speed  wonder    7  withdraw  gold  green  \\\n",
       "0     ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "1     ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "2     ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "3     ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "4     ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "...   ...    ...      ...   ...    ...     ...  ...       ...   ...    ...   \n",
       "1142  ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "1143  ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "1144  ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "1145  ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "1146  ...    0.0      0.0   0.0    0.0     0.0  0.0       0.0   0.0    0.0   \n",
       "\n",
       "      silenc  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "1142     0.0  \n",
       "1143     0.0  \n",
       "1144     0.0  \n",
       "1145     0.0  \n",
       "1146     0.0  \n",
       "\n",
       "[1147 rows x 2697 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
