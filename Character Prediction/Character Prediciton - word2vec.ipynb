{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Prediction\n",
    "\n",
    "## Word to Vector and Bag of Words dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data \n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "['ARAGORN', 'FARAMIR', 'FRODO', 'GANDALF', 'GIMLI', 'GOLLUM', 'MERRY', 'PIPPIN', 'SAM', 'THEODEN']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for y dataset and common labels\n",
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"ARAGORN\"]\n",
    "\n",
    "def common_label_removal(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "def y_split(data):\n",
    "    y_data = data['char']\n",
    "    return y_data\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    data = common_label_removal(data)\n",
    "    y_data = y_split(data)\n",
    "    y_data = char_2_num(y_data)\n",
    "    return pd.Series(y_data)\n",
    "\n",
    "train_y = preprocessing(train_df)\n",
    "val_y = preprocessing(train_df)\n",
    "test_y = preprocessing(train_df)\n",
    "\n",
    "train_y.to_csv('train_y.csv', index=False)\n",
    "val_y.to_csv('val_y.csv', index=False)\n",
    "test_y.to_csv('test_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vector Dataset\n",
    "\n",
    "This idea was ultimatrly not used but was still apart of our research process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def find_max_length(data):\n",
    "#     return max(max(len(nlp(dialogue)) for dialogue in set) for set in data)\n",
    "\n",
    "# max_length = find_max_length([train_df['dialog'], val_df['dialog'], test_df['dialog']])\n",
    "\n",
    "# def word2vec_seq(data, max_length):\n",
    "#     # Extracting norm vector values\n",
    "#     word_vectors = []\n",
    "#     for dialogue in data['dialog']:\n",
    "#         tokens = nlp(dialogue)\n",
    "#         dialogue_vectors = [token.vector_norm for token in tokens]\n",
    "#         word_vectors.append(dialogue_vectors)\n",
    "\n",
    "#     # Padding \n",
    "#     for i in range(len(word_vectors)):\n",
    "#         word_vectors[i] += [100] * (max_length - len(word_vectors[i]))\n",
    "\n",
    "#     df = pd.DataFrame(word_vectors)\n",
    "#     df.columns = [f\"word_{i}\" for i in range(1, max_length + 1)]\n",
    "\n",
    "#     df = pd.concat([data, df], axis=1)\n",
    "#     return df  \n",
    "\n",
    "# train_B = word2vec_seq(train_df, max_length)\n",
    "# val_B = word2vec_seq(val_df, max_length)\n",
    "# test_B = word2vec_seq(test_df, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_B.to_csv('train_B.csv', index=False)\n",
    "# val_B.to_csv('val_B.csv', index=False)\n",
    "# test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = common_label_removal(train_df).reset_index(drop=True)\n",
    "val_B = common_label_removal(val_df).reset_index(drop=True)\n",
    "test_B = common_label_removal(test_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_list(X):\n",
    "    quote_list = []\n",
    "    for quote in range(len(X)):\n",
    "        splitted_quote =  X['dialog'][quote].split()\n",
    "        sequence_list = []\n",
    "        for split in range(len(splitted_quote)):\n",
    "            splitted_word = splitted_quote[split]\n",
    "\n",
    "            word_list = str()\n",
    "            i=0\n",
    "            while i < (len(splitted_word)):\n",
    "                # print(splitted_word[i])|\n",
    "                if splitted_word[i].isalpha() == True:\n",
    "                    word_list += splitted_word[i]\n",
    "                i+=1\n",
    "            sequence_list.append(word_list)\n",
    "        quote_list.append(sequence_list)\n",
    "    return quote_list\n",
    "\n",
    "B1 = quote_list(train_B)\n",
    "B2 = quote_list(val_B)\n",
    "B3 = quote_list(test_B)\n",
    "\n",
    "def maxlen(X):\n",
    "    uni = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[i])):\n",
    "            if X[i][j] not in uni:\n",
    "                uni.append(X[i][j])\n",
    "    return len(uni)\n",
    "max_length = maxlen(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(B1)\n",
    "B1_seq = tokenizer.texts_to_sequences(B1)\n",
    "B2_seq = tokenizer.texts_to_sequences(B2)\n",
    "B3_seq = tokenizer.texts_to_sequences(B3)\n",
    "maxlen = max([len(seq) for seq in B1_seq])\n",
    "\n",
    "B1_padseq = pad_sequences(B1_seq, maxlen=max_length,padding='post')\n",
    "B2_padseq = pad_sequences(B2_seq, maxlen=maxlen,padding='post')\n",
    "B3_padseq = pad_sequences(B3_seq, maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B = pd.DataFrame(B1_padseq)\n",
    "val_B = pd.DataFrame(B2_padseq)\n",
    "test_B = pd.DataFrame(B3_padseq)\n",
    "\n",
    "train_B.to_csv('train_B.csv', index=False)\n",
    "val_B.to_csv('val_B.csv', index=False)\n",
    "test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Removing; punctuation, stopwords and stemming words\n",
    "def bow_preprocess(df):\n",
    "    preprocessed = []\n",
    "    \n",
    "    for dialog in df['dialog']:\n",
    "        tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \", dialog).split()\n",
    "        alfa = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords_set]\n",
    "        bravo = str()\n",
    "        for i in range(len(alfa)):\n",
    "            bravo += str(alfa[i])\n",
    "            bravo += ' '\n",
    "        preprocessed.append(bravo)\n",
    "\n",
    "    df['dialog'] = preprocessed\n",
    "    return df\n",
    "\n",
    "\n",
    "train_C = bow_preprocess(train_df)\n",
    "val_C = bow_preprocess(val_df)\n",
    "test_C = bow_preprocess(test_df)\n",
    "\n",
    "# Creating a set of unique words amongst all dialoges\n",
    "all_dialogs = pd.concat([train_C['dialog'], val_C['dialog'], test_C['dialog']], ignore_index=True)\n",
    "wordset = set([word for dialog in all_dialogs for word in dialog.split()])\n",
    "\n",
    "# Creating dataframe\n",
    "def BOW_df(wordset, df):\n",
    "\n",
    "    bow_df = pd.DataFrame(columns=list(wordset))\n",
    "    \n",
    "    for i, dialog in enumerate(df['dialog']):\n",
    "        dialog_tf_diz = {}\n",
    "        for word in dialog.split():\n",
    "            if word in wordset:\n",
    "                if word in dialog_tf_diz:\n",
    "                    dialog_tf_diz[word] += 1\n",
    "                else:\n",
    "                    dialog_tf_diz[word] = 1\n",
    "        \n",
    "        bow_df.loc[i] = [dialog_tf_diz.get(word, 0) for word in wordset]\n",
    "    return pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "train_C = BOW_df(wordset, train_C)\n",
    "val_C = BOW_df(wordset, val_C)\n",
    "test_C = BOW_df(wordset, test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_C = common_label_removal(train_C).reset_index(drop=True).iloc[:,2:]\n",
    "val_C = common_label_removal(val_C).reset_index(drop=True).iloc[:,2:]\n",
    "test_C = common_label_removal(test_C).reset_index(drop=True).iloc[:,2:]\n",
    "\n",
    "train_C.to_csv('train_C.csv', index=False)\n",
    "val_C.to_csv('val_C.csv', index=False)\n",
    "test_C.to_csv('test_C.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf_idf__df(df):\n",
    "    preprocessed_dialogs = bow_preprocess(df)\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=wordset)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_dialogs['dialog'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=list(wordset))\n",
    "    return pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "train_D = tf_idf__df(train_df)\n",
    "val_D = tf_idf__df(val_df)\n",
    "test_D = tf_idf__df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = common_label_removal(train_D).reset_index(drop=True).iloc[:,2:]\n",
    "val_D = common_label_removal(val_D).reset_index(drop=True).iloc[:,2:]\n",
    "test_D = common_label_removal(test_D).reset_index(drop=True).iloc[:,2:]\n",
    "\n",
    "train_D.to_csv('train_D.csv', index=False)\n",
    "val_D.to_csv('val_D.csv', index=False)\n",
    "test_D.to_csv('test_D.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
