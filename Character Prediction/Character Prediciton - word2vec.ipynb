{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Prediction\n",
    "\n",
    "## Word to Vector and Bag of Words dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data \n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vector Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def find_max_length(data):\n",
    "#     return max(max(len(nlp(dialogue)) for dialogue in set) for set in data)\n",
    "\n",
    "# max_length = find_max_length([train_df['dialog'], val_df['dialog'], test_df['dialog']])\n",
    "\n",
    "# def word2vec_seq(data, max_length):\n",
    "#     # Extracting norm vector values\n",
    "#     word_vectors = []\n",
    "#     for dialogue in data['dialog']:\n",
    "#         tokens = nlp(dialogue)\n",
    "#         dialogue_vectors = [token.vector_norm for token in tokens]\n",
    "#         word_vectors.append(dialogue_vectors)\n",
    "\n",
    "#     # Padding \n",
    "#     for i in range(len(word_vectors)):\n",
    "#         word_vectors[i] += [100] * (max_length - len(word_vectors[i]))\n",
    "\n",
    "#     df = pd.DataFrame(word_vectors)\n",
    "#     df.columns = [f\"word_{i}\" for i in range(1, max_length + 1)]\n",
    "\n",
    "#     df = pd.concat([data, df], axis=1)\n",
    "#     return df  \n",
    "\n",
    "# train_B = word2vec_seq(train_df, max_length)\n",
    "# val_B = word2vec_seq(val_df, max_length)\n",
    "# test_B = word2vec_seq(test_df, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_B.to_csv('train_B.csv', index=False)\n",
    "# val_B.to_csv('val_B.csv', index=False)\n",
    "# test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Removing; punctuation, stopwords and stemming words\n",
    "def bow_preprocess(df):\n",
    "    preprocessed = []\n",
    "    \n",
    "    for dialog in df['dialog']:\n",
    "        tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \", dialog).split()\n",
    "        alfa = [stemmer.stem(word.lower()) for word in tokens if word not in stopwords_set]\n",
    "        bravo = str()\n",
    "        for i in range(len(alfa)):\n",
    "            bravo += str(alfa[i])\n",
    "            bravo += ' '\n",
    "        preprocessed.append(bravo)\n",
    "\n",
    "    df['dialog'] = preprocessed\n",
    "    return df\n",
    "\n",
    "\n",
    "train_C = bow_preprocess(train_df)\n",
    "val_C = bow_preprocess(val_df)\n",
    "test_C = bow_preprocess(test_df)\n",
    "\n",
    "# Creating a set of unique words amongst all dialoges\n",
    "all_dialogs = pd.concat([train_C['dialog'], val_C['dialog'], test_C['dialog']], ignore_index=True)\n",
    "wordset = set([word for dialog in all_dialogs for word in dialog.split()])\n",
    "\n",
    "# Creating dataframe\n",
    "def BOW_df(wordset, df):\n",
    "\n",
    "    bow_df = pd.DataFrame(columns=list(wordset))\n",
    "    \n",
    "    for i, dialog in enumerate(df['dialog']):\n",
    "        dialog_tf_diz = {}\n",
    "        for word in dialog.split():\n",
    "            if word in wordset:\n",
    "                if word in dialog_tf_diz:\n",
    "                    dialog_tf_diz[word] += 1\n",
    "                else:\n",
    "                    dialog_tf_diz[word] = 1\n",
    "        \n",
    "        bow_df.loc[i] = [dialog_tf_diz.get(word, 0) for word in wordset]\n",
    "    return pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "train_C = BOW_df(wordset, train_C)\n",
    "val_C = BOW_df(wordset, val_C)\n",
    "test_C = BOW_df(wordset, test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_C.to_csv('train_C.csv', index=False)\n",
    "val_C.to_csv('val_C.csv', index=False)\n",
    "test_C.to_csv('test_C.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tf_idf__df(df):\n",
    "    preprocessed_dialogs = bow_preprocess(df)\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=wordset)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_dialogs['dialog'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=list(wordset))\n",
    "    return pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "train_D = tf_idf__df(train_df)\n",
    "val_D = tf_idf__df(val_df)\n",
    "test_D = tf_idf__df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D.to_csv('train_D.csv', index=False)\n",
    "val_D.to_csv('val_D.csv', index=False)\n",
    "test_D.to_csv('test_D.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
