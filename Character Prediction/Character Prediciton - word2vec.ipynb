{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Prediction\n",
    "\n",
    "## Word to Vector and Bag of Words dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data \n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word 2 Vector Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def find_max_length(data):\n",
    "    return max(max(len(nlp(dialogue)) for dialogue in set) for set in data)\n",
    "\n",
    "max_length = find_max_length([train_df['dialog'], val_df['dialog'], test_df['dialog']])\n",
    "\n",
    "def word2vec_seq(data, max_length):\n",
    "    # Extracting norm vector values\n",
    "    word_vectors = []\n",
    "    for dialogue in data['dialog']:\n",
    "        tokens = nlp(dialogue)\n",
    "        dialogue_vectors = [token.vector_norm for token in tokens]\n",
    "        word_vectors.append(dialogue_vectors)\n",
    "\n",
    "    # Padding \n",
    "    for i in range(len(word_vectors)):\n",
    "        word_vectors[i] += [100] * (max_length - len(word_vectors[i]))\n",
    "\n",
    "    df = pd.DataFrame(word_vectors)\n",
    "    df.columns = [f\"word_{i}\" for i in range(1, max_length + 1)]\n",
    "\n",
    "    df = pd.concat([data, df], axis=1)\n",
    "    return df  \n",
    "\n",
    "train_B = word2vec_seq(train_df, max_length)\n",
    "val_B = word2vec_seq(val_df, max_length)\n",
    "test_B = word2vec_seq(test_df, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_B.to_csv('train_B.csv', index=False)\n",
    "val_B.to_csv('val_B.csv', index=False)\n",
    "test_B.to_csv('test_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Removing; punctuation, stopwords and stemming words\n",
    "def bow_preprocess(df):\n",
    "    preprocessed = []\n",
    "    \n",
    "    for dialog in df['dialog']:\n",
    "        tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \", dialog.lower()).split()\n",
    "        preprocessed.append([stemmer.stem(word) for word in tokens if word not in stopwords_set])\n",
    "\n",
    "    df['dialog'] = preprocessed\n",
    "    return df\n",
    "\n",
    "\n",
    "train_C = bow_preprocess(train_df)\n",
    "val_C = bow_preprocess(val_df)\n",
    "test_C = bow_preprocess(test_df)\n",
    "\n",
    "# Creating a set of unique words amongst all dialoges\n",
    "all_dialogs = pd.concat([train_C['dialog'], val_C['dialog'], test_C['dialog']], ignore_index=True)\n",
    "wordset = set([word for dialog in all_dialogs for word in dialog])\n",
    "\n",
    "# Creating dataframe\n",
    "def BOW_df(wordset, df):\n",
    "\n",
    "    bow_df = pd.DataFrame(columns=list(wordset))\n",
    "    \n",
    "    for i, dialog in enumerate(df['dialog']):\n",
    "        dialog_tf_diz = {}\n",
    "        \n",
    "        for word in dialog:\n",
    "            if word in wordset:\n",
    "                if word in dialog_tf_diz:\n",
    "                    dialog_tf_diz[word] += 1\n",
    "                else:\n",
    "                    dialog_tf_diz[word] = 1\n",
    "        \n",
    "        bow_df.loc[i] = [dialog_tf_diz.get(word, 0) for word in wordset]\n",
    "    return pd.concat([df, bow_df], axis=1)\n",
    "\n",
    "train_C = BOW_df(wordset, train_C)\n",
    "val_C = BOW_df(wordset, val_C)\n",
    "test_C = BOW_df(wordset, test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_C.to_csv('train_C.csv', index=False)\n",
    "val_C.to_csv('val_C.csv', index=False)\n",
    "test_C.to_csv('test_C.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
