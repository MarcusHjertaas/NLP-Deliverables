{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_train = pd.read_csv('lotr_train.csv')\n",
    "lotr_test = pd.read_csv('lotr_test.csv')\n",
    "\n",
    "imp_char = [\"FRODO\", \"SAM\", \"GANDALF\", \"PIPPIN\", \"MERRY\", \"GOLLUM\", \"GIMLI\", \"THEODEN\", \"FARAMIR\", \"SAURON\", \"ARAGORN\", \"SMEAGOL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character prediction\n",
    "### Divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a common label for the characters not of interest. \n",
    "# Aware that this will impact the model, unsure if it would be positive or negative\n",
    "\n",
    "\n",
    "def common_label(data):\n",
    "    mask = data[\"char\"].isin(imp_char)\n",
    "    data.loc[~ mask, \"char\"] = \"Rest\"\n",
    "    mask2 = data['char'] == 'Rest'\n",
    "    data = data[~mask2]\n",
    "    return data\n",
    "\n",
    "lotr_train = common_label(lotr_train)\n",
    "lotr_test = common_label(lotr_test)\n",
    "\n",
    "\n",
    "def x_y_split(data):\n",
    "    y_data = data['char']\n",
    "    x_data = data.drop(columns=['char', 'dialog'])\n",
    "    return x_data, y_data\n",
    "\n",
    "lotr_train_X, lotr_train_Y = x_y_split(lotr_train)\n",
    "lotr_test_X, lotr_test_Y = x_y_split(lotr_test)\n",
    "\n",
    "def char_2_num(y_data):\n",
    "    encoder = LabelEncoder()\n",
    "    y_data = y_data.values.reshape(-1, 1)\n",
    "    encoded_data = encoder.fit_transform(y_data)\n",
    "    names = list(encoder.inverse_transform(np.unique(encoded_data)))\n",
    "    print(names)\n",
    "    print(np.unique(encoded_data))\n",
    "    return encoded_data, names\n",
    "\n",
    "lotr_train_Y, names = char_2_num(lotr_train_Y)\n",
    "lotr_test_Y = char_2_num(lotr_test_Y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_methods = [ f1_score, precision_score, recall_score]\n",
    "\n",
    "def naive_model(x_data, y_data):\n",
    "    pred = np.random.randint(0, 12, size=len(x_data))\n",
    "    print(classification_report(y_data, pred))\n",
    "    print(\"Accuracy \", round(accuracy_score(y_data, pred), 4)) \n",
    "    \n",
    "    for e in eval_methods:\n",
    "        print(str(e.__name__), round(e(y_data, pred, average='weighted'), 4)) \n",
    "    return pred   \n",
    "\n",
    "naive_predicitons = naive_model(lotr_test_X, lotr_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model_parameters(X_train, y_train, model, param_grid, cv=5):\n",
    " \n",
    "    rfc = model()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Accuracy Score:\", best_score)\n",
    "\n",
    "\n",
    "    optimized = model(**best_params)\n",
    "    optimized.fit(X_train, y_train)\n",
    "    return optimized\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [15, 20, 25],\n",
    "    'max_depth': [None, 3, 4, 5],\n",
    "    'min_samples_split': [ 5, 10, 15],\n",
    "    #'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Optimize parameters\n",
    "optimized_rf = optimize_model_parameters(lotr_train_X, lotr_train_Y, RFC, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_data, y_data, model):\n",
    "\n",
    "    pred = model.predict(x_data)\n",
    "    print(classification_report(y_data, pred))\n",
    "    print(\"Accuracy \", round(accuracy_score(y_data, pred), 4)) \n",
    "    \n",
    "    for e in eval_methods:\n",
    "        print(str(e.__name__), round(e(y_data, pred, average='weighted'), 4)) \n",
    "    return pred  \n",
    "\n",
    "rfc_predictions = evaluate_model(lotr_test_X, lotr_test_Y, optimized_rf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conf_matrix(y, pred):\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    fig, ax = plt.subplots(figsize=(10,10)) \n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, \n",
    "                fmt='.1%', cmap='Blues', ax=ax, \n",
    "                xticklabels=names, yticklabels=names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.show()\n",
    "\n",
    "conf_matrix(lotr_test_Y, rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
